{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Maze with Q-learning  ( Reinforcement Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maze generator consists of maze class and field class. It generates a square shaped maze. The maze has route tiles, wall and block tiles, starting and goal point. The route tiles have -1 or 0 on it, which is the point you can get by stepping it. Apparently you will get 1 point subtracted if you step on -1 tile. The wall and block tiles, in #, are where you cannot interude. You have to bypass #. The starting point, namely S, is where you start the maze and goal point, which is shown as 50, is where you head to. You will earn 50 points when you made to the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective of this notebook is to solve self-made maze with Q-learning.\n",
    "### The maze is in square shape, consists of start point, goal point and tiles in the mid of them.\n",
    "### Each tile has numericals as its point. In other words, if you step on to the tile with -1, you get 1 point subtracted.\n",
    "### The maze has blocks to prevent you from taking the route."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pds\n",
    "import random\n",
    "import copy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from collections import deque\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maze Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze(object):\n",
    "    def __init__(self, size=10, blocks_rate=0.1):\n",
    "        self.size = size if size > 3 else 10\n",
    "        self.blocks = int((size ** 2) * blocks_rate) \n",
    "        self.s_list = []\n",
    "        self.maze_list = []\n",
    "        self.e_list = []\n",
    "\n",
    "    def create_mid_lines(self, k):\n",
    "        if k == 0: self.maze_list.append(self.s_list)\n",
    "        elif k == self.size - 1: self.maze_list.append(self.e_list)\n",
    "        else:\n",
    "            tmp_list = []\n",
    "            for l in range(0,self.size):\n",
    "                if l == 0: tmp_list.extend(\"#\")\n",
    "                elif l == self.size-1: tmp_list.extend(\"#\")\n",
    "                else:\n",
    "                    a = random.randint(-1, 0)\n",
    "                    tmp_list.extend([a])\n",
    "            self.maze_list.append(tmp_list)\n",
    "\n",
    "    def insert_blocks(self, k, s_r, e_r):\n",
    "        b_y = random.randint(1, self.size-2)\n",
    "        b_x = random.randint(1, self.size-2)\n",
    "        if [b_y, b_x] == [1, s_r] or [b_y, b_x] == [self.size - 2, e_r]: k = k-1\n",
    "        else: self.maze_list[b_y][b_x] = \"#\"\n",
    "            \n",
    "    def generate_maze(self): \n",
    "        s_r = random.randint(1, (self.size / 2) - 1)\n",
    "        for i in range(0, self.size):\n",
    "            if i == s_r: self.s_list.extend(\"S\")\n",
    "            else: self.s_list.extend(\"#\")\n",
    "        start_point = [0, s_r]\n",
    "\n",
    "        e_r = random.randint((self.size / 2) + 1, self.size - 2)\n",
    "        for j in range(0, self.size):\n",
    "            if j == e_r: self.e_list.extend([50])\n",
    "            else: self.e_list.extend(\"#\")\n",
    "        goal_point = [self.size - 1, e_r]\n",
    "\n",
    "        for k in range(0, self.size):\n",
    "            self.create_mid_lines(k)\n",
    "        \n",
    "        for k in range(self.blocks):\n",
    "            self.insert_blocks(k, s_r, e_r)\n",
    "\n",
    "        return self.maze_list, start_point, goal_point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maze functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Field(object):\n",
    "    def __init__(self, maze, start_point, goal_point):\n",
    "        self.maze = maze\n",
    "        self.start_point = start_point\n",
    "        self.goal_point = goal_point\n",
    "        self.movable_vec = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "\n",
    "    def display(self, point=None):\n",
    "        field_data = copy.deepcopy(self.maze)\n",
    "        if not point is None:\n",
    "                y, x = point\n",
    "                field_data[y][x] = \"@@\"\n",
    "        else:\n",
    "                point = \"\"\n",
    "        for line in field_data:\n",
    "                print (\"\\t\" + \"%3s \" * len(line) % tuple(line))\n",
    "\n",
    "    def get_actions(self, state):\n",
    "        movables = []\n",
    "        if state == self.start_point:\n",
    "            y = state[0] + 1\n",
    "            x = state[1]\n",
    "            a = [[y, x]]\n",
    "            return a\n",
    "        else:\n",
    "            for v in self.movable_vec:\n",
    "                y = state[0] + v[0]\n",
    "                x = state[1] + v[1]\n",
    "                if not(0 < x < len(self.maze) and\n",
    "                       0 <= y <= len(self.maze) - 1 and\n",
    "                       maze[y][x] != \"#\" and\n",
    "                       maze[y][x] != \"S\"):\n",
    "                    continue\n",
    "                movables.append([y,x])\n",
    "            if len(movables) != 0:\n",
    "                return movables\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "    def get_val(self, state):\n",
    "        y, x = state\n",
    "        if state == self.start_point: return 0, False\n",
    "        else:\n",
    "            v = float(self.maze[y][x])\n",
    "            if state == self.goal_point: \n",
    "                return v, True\n",
    "            else: \n",
    "                return v, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 10\n",
    "barriar_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze_1 = Maze(size, barriar_rate)\n",
    "maze, start_point, goal_point = maze_1.generate_maze()\n",
    "maze_field = Field(maze, start_point, goal_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t  #   #   #   #   S   #   #   #   #   # \n",
      "\t  #  -1   0   0  -1   0  -1   #  -1   # \n",
      "\t  #  -1  -1   0  -1  -1  -1   #  -1   # \n",
      "\t  #  -1  -1  -1  -1  -1  -1   #  -1   # \n",
      "\t  #   #  -1   #   0   0   0  -1  -1   # \n",
      "\t  #   0  -1  -1   0  -1   0   #   0   # \n",
      "\t  #  -1   0   0  -1   0  -1  -1   0   # \n",
      "\t  #   #  -1   #   0  -1  -1  -1  -1   # \n",
      "\t  #  -1   0  -1  -1   0   0   #   0   # \n",
      "\t  #   #   #   #   #   #   #   #  50   # \n"
     ]
    }
   ],
   "source": [
    "maze_field.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Solving the maze in Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning_Solver(object):\n",
    "    def __init__(self, maze, display=False):\n",
    "        self.Qvalue = {}\n",
    "        self.Field = maze\n",
    "        self.alpha = 0.2\n",
    "        self.gamma  = 0.9\n",
    "        self.epsilon = 0.2\n",
    "        self.steps = 0\n",
    "        self.score = 0\n",
    "        self.display = display\n",
    "\n",
    "    def qlearn(self, greedy_flg=False):\n",
    "        state = self.Field.start_point\n",
    "        while True:\n",
    "            if greedy_flg:\n",
    "                self.steps += 1\n",
    "                action = self.choose_action_greedy(state)\n",
    "                print(\"current state: {0} -> action: {1} \".format(state, action))\n",
    "                if self.display:\n",
    "                    self.Field.display(action)\n",
    "                reward, tf = self.Field.get_val(action)\n",
    "                self.score =  self.score + reward\n",
    "                print(\"current step: {0} \\t score: {1}\\n\".format(self.steps, self.score))\n",
    "                if tf == True:\n",
    "                    print(\"Goal!\")\n",
    "                    break\n",
    "            else:\n",
    "                action = self.choose_action(state)    \n",
    "            if self.update_Qvalue(state, action):\n",
    "                break\n",
    "            else:\n",
    "                state = action\n",
    "\n",
    "    def update_Qvalue(self, state, action):\n",
    "        Q_s_a = self.get_Qvalue(state, action)\n",
    "        mQ_s_a = max([self.get_Qvalue(action, n_action) for n_action in self.Field.get_actions(action)])\n",
    "        r_s_a, finish_flg = self.Field.get_val(action)\n",
    "        q_value = Q_s_a + self.alpha * ( r_s_a +  self.gamma * mQ_s_a - Q_s_a)\n",
    "        self.set_Qvalue(state, action, q_value)\n",
    "        return finish_flg\n",
    "\n",
    "\n",
    "    def get_Qvalue(self, state, action):\n",
    "        state = (state[0],state[1])\n",
    "        action = (action[0],action[1])\n",
    "        try:\n",
    "            return self.Qvalue[state][action]\n",
    "        except KeyError:\n",
    "            return 0.0\n",
    "\n",
    "    def set_Qvalue(self, state, action, q_value):\n",
    "        state = (state[0],state[1])\n",
    "        action = (action[0],action[1])\n",
    "        self.Qvalue.setdefault(state,{})\n",
    "        self.Qvalue[state][action] = q_value\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if self.epsilon < random.random():\n",
    "            return random.choice(self.Field.get_actions(state))\n",
    "        else:\n",
    "            return self.choose_action_greedy(state)\n",
    "\n",
    "    def choose_action_greedy(self, state):\n",
    "        best_actions = []\n",
    "        max_q_value = -100\n",
    "        for a in self.Field.get_actions(state):\n",
    "            q_value = self.get_Qvalue(state, a)\n",
    "            if q_value > max_q_value:\n",
    "                best_actions = [a,]\n",
    "                max_q_value = q_value\n",
    "            elif q_value == max_q_value:\n",
    "                best_actions.append(a)\n",
    "        return random.choice(best_actions)\n",
    "\n",
    "    def dump_Qvalue(self):\n",
    "        print(\"##### Dump Qvalue #####\")\n",
    "        for i, s in enumerate(self.Qvalue.keys()):\n",
    "            for a in self.Qvalue[s].keys():\n",
    "                print(\"\\t\\tQ(s, a): Q(%s, %s): %s\" % (str(s), str(a), str(self.Qvalue[s][a])))\n",
    "            if i != len(self.Qvalue.keys())-1: \n",
    "                print('\\t------------------state   action   reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_count = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "QL_solver = QLearning_Solver(maze_field, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(learning_count):\n",
    "    QL_solver.qlearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Dump Qvalue #####\n",
      "\t\tQ(s, a): Q((0, 4), (1, 4)): 10.32503427394995\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((1, 4), (1, 3)): 10.038630846554952\n",
      "\t\tQ(s, a): Q((1, 4), (1, 5)): 12.583371415499947\n",
      "\t\tQ(s, a): Q((1, 4), (2, 4)): 12.393371415499947\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((1, 3), (1, 2)): 9.034767761899454\n",
      "\t\tQ(s, a): Q((1, 3), (1, 4)): 10.32503427394995\n",
      "\t\tQ(s, a): Q((1, 3), (2, 3)): 11.15403427394995\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((1, 2), (2, 2)): 9.038630846554918\n",
      "\t\tQ(s, a): Q((1, 2), (1, 1)): 7.131290985560396\n",
      "\t\tQ(s, a): Q((1, 2), (1, 3)): 10.038630846554952\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((2, 2), (1, 2)): 9.0347677618988\n",
      "\t\tQ(s, a): Q((2, 2), (3, 2)): 10.154034273945033\n",
      "\t\tQ(s, a): Q((2, 2), (2, 1)): 7.134767761888524\n",
      "\t\tQ(s, a): Q((2, 2), (2, 3)): 11.15403427394995\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((1, 1), (1, 2)): 9.034767761879941\n",
      "\t\tQ(s, a): Q((1, 1), (2, 1)): 7.134767760708857\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((2, 1), (1, 1)): 7.1312908560673\n",
      "\t\tQ(s, a): Q((2, 1), (3, 1)): 8.138630835958912\n",
      "\t\tQ(s, a): Q((2, 1), (2, 2)): 9.038630846554913\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((1, 5), (1, 4)): 10.32503427394995\n",
      "\t\tQ(s, a): Q((1, 5), (2, 5)): 13.981523794999944\n",
      "\t\tQ(s, a): Q((1, 5), (1, 6)): 13.171523794999949\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((2, 4), (2, 3)): 11.15403427394995\n",
      "\t\tQ(s, a): Q((2, 4), (3, 4)): 14.881523794999945\n",
      "\t\tQ(s, a): Q((2, 4), (2, 5)): 13.981523794999944\n",
      "\t\tQ(s, a): Q((2, 4), (1, 4)): 10.32503427394995\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((2, 3), (1, 3)): 10.038630846554952\n",
      "\t\tQ(s, a): Q((2, 3), (2, 4)): 12.393371415499947\n",
      "\t\tQ(s, a): Q((2, 3), (3, 3)): 12.393371415499947\n",
      "\t\tQ(s, a): Q((2, 3), (2, 2)): 9.038630846554952\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((3, 2), (3, 1)): 8.138630846531719\n",
      "\t\tQ(s, a): Q((3, 2), (2, 2)): 9.038630846536389\n",
      "\t\tQ(s, a): Q((3, 2), (3, 3)): 12.393371415499947\n",
      "\t\tQ(s, a): Q((3, 2), (4, 2)): 11.737271385809324\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((3, 1), (2, 1)): 7.134767596897525\n",
      "\t\tQ(s, a): Q((3, 1), (3, 2)): 10.154034273949737\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((3, 3), (3, 4)): 14.881523794999945\n",
      "\t\tQ(s, a): Q((3, 3), (3, 2)): 10.15403427394995\n",
      "\t\tQ(s, a): Q((3, 3), (2, 3)): 11.15403427394995\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((3, 4), (3, 5)): 16.646137549999942\n",
      "\t\tQ(s, a): Q((3, 4), (3, 3)): 12.393371415499947\n",
      "\t\tQ(s, a): Q((3, 4), (2, 4)): 12.393371415499947\n",
      "\t\tQ(s, a): Q((3, 4), (4, 4)): 17.646137549999942\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((3, 5), (3, 4)): 14.881523794999945\n",
      "\t\tQ(s, a): Q((3, 5), (4, 5)): 19.606819499999943\n",
      "\t\tQ(s, a): Q((3, 5), (2, 5)): 13.981523794999944\n",
      "\t\tQ(s, a): Q((3, 5), (3, 6)): 18.606819499999943\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((2, 5), (2, 6)): 15.746137549999947\n",
      "\t\tQ(s, a): Q((2, 5), (1, 5)): 12.583371415499947\n",
      "\t\tQ(s, a): Q((2, 5), (3, 5)): 16.646137549999942\n",
      "\t\tQ(s, a): Q((2, 5), (2, 4)): 12.393371415499947\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((2, 6), (2, 5)): 13.981523794999944\n",
      "\t\tQ(s, a): Q((2, 6), (1, 6)): 13.171523794999949\n",
      "\t\tQ(s, a): Q((2, 6), (3, 6)): 18.606819499999943\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((1, 6), (1, 5)): 12.583371415499947\n",
      "\t\tQ(s, a): Q((1, 6), (2, 6)): 15.746137549999947\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((4, 5), (4, 6)): 21.785354999999946\n",
      "\t\tQ(s, a): Q((4, 5), (5, 5)): 20.785354999999946\n",
      "\t\tQ(s, a): Q((4, 5), (3, 5)): 16.646137549999942\n",
      "\t\tQ(s, a): Q((4, 5), (4, 4)): 17.646137549999942\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((4, 6), (3, 6)): 18.606819499999943\n",
      "\t\tQ(s, a): Q((4, 6), (4, 7)): 24.01594999999995\n",
      "\t\tQ(s, a): Q((4, 6), (5, 6)): 24.205949999999948\n",
      "\t\tQ(s, a): Q((4, 6), (4, 5)): 19.606819499999943\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((3, 6), (3, 5)): 16.646137549999942\n",
      "\t\tQ(s, a): Q((3, 6), (2, 6)): 15.746137549999947\n",
      "\t\tQ(s, a): Q((3, 6), (4, 6)): 21.785354999999946\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((4, 2), (5, 2)): 14.152523794266182\n",
      "\t\tQ(s, a): Q((4, 2), (3, 2)): 10.154034111282787\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((5, 2), (6, 2)): 16.836137549999943\n",
      "\t\tQ(s, a): Q((5, 2), (5, 1)): 12.737270656944519\n",
      "\t\tQ(s, a): Q((5, 2), (5, 3)): 15.836137549984736\n",
      "\t\tQ(s, a): Q((5, 2), (4, 2)): 11.737270071772851\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((6, 2), (7, 2)): 14.152523794806218\n",
      "\t\tQ(s, a): Q((6, 2), (5, 2)): 14.152523793884638\n",
      "\t\tQ(s, a): Q((6, 2), (6, 1)): 14.152523790204249\n",
      "\t\tQ(s, a): Q((6, 2), (6, 3)): 18.706819499999945\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((7, 2), (8, 2)): 16.060795148897938\n",
      "\t\tQ(s, a): Q((7, 2), (6, 2)): 16.8361375499854\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((8, 2), (7, 2)): 14.15250905438783\n",
      "\t\tQ(s, a): Q((8, 2), (8, 1)): 13.454616703741161\n",
      "\t\tQ(s, a): Q((8, 2), (8, 3)): 17.8453294994506\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((8, 1), (8, 2)): 16.060789909555005\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((8, 3), (8, 4)): 20.939254999999946\n",
      "\t\tQ(s, a): Q((8, 3), (8, 2)): 16.060796511438056\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((8, 4), (8, 3)): 17.84532949999956\n",
      "\t\tQ(s, a): Q((8, 4), (7, 4)): 23.47694999999995\n",
      "\t\tQ(s, a): Q((8, 4), (8, 5)): 24.376949999999947\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((5, 1), (6, 1)): 14.15250921673073\n",
      "\t\tQ(s, a): Q((5, 1), (5, 2)): 14.15252377185665\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((6, 1), (6, 2)): 16.83613754979498\n",
      "\t\tQ(s, a): Q((6, 1), (5, 1)): 12.737265605597212\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((5, 3), (6, 3)): 18.706819499999167\n",
      "\t\tQ(s, a): Q((5, 3), (5, 2)): 14.152523794999524\n",
      "\t\tQ(s, a): Q((5, 3), (5, 4)): 18.706819499999945\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((6, 3), (6, 4)): 20.785354999999946\n",
      "\t\tQ(s, a): Q((6, 3), (5, 3)): 15.836137549999947\n",
      "\t\tQ(s, a): Q((6, 3), (6, 2)): 16.836137549999943\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((6, 4), (5, 4)): 18.706819499999945\n",
      "\t\tQ(s, a): Q((6, 4), (6, 3)): 18.706819499999945\n",
      "\t\tQ(s, a): Q((6, 4), (6, 5)): 24.205949999999948\n",
      "\t\tQ(s, a): Q((6, 4), (7, 4)): 23.47694999999995\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((5, 4), (6, 4)): 20.785354999999946\n",
      "\t\tQ(s, a): Q((5, 4), (4, 4)): 17.646137549999942\n",
      "\t\tQ(s, a): Q((5, 4), (5, 5)): 20.785354999999946\n",
      "\t\tQ(s, a): Q((5, 4), (5, 3)): 15.836137549999947\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((6, 5), (5, 5)): 20.785354999999946\n",
      "\t\tQ(s, a): Q((6, 5), (6, 6)): 26.89549999999995\n",
      "\t\tQ(s, a): Q((6, 5), (6, 4)): 20.785354999999946\n",
      "\t\tQ(s, a): Q((6, 5), (7, 5)): 26.08549999999995\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((5, 5), (6, 5)): 24.205949999999948\n",
      "\t\tQ(s, a): Q((5, 5), (5, 6)): 24.205949999999948\n",
      "\t\tQ(s, a): Q((5, 5), (4, 5)): 19.606819499999943\n",
      "\t\tQ(s, a): Q((5, 5), (5, 4)): 18.706819499999945\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((6, 6), (6, 7)): 30.99499999999995\n",
      "\t\tQ(s, a): Q((6, 6), (5, 6)): 24.205949999999948\n",
      "\t\tQ(s, a): Q((6, 6), (7, 6)): 30.094999999999953\n",
      "\t\tQ(s, a): Q((6, 6), (6, 5)): 24.205949999999948\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((6, 7), (6, 6)): 26.89549999999995\n",
      "\t\tQ(s, a): Q((6, 7), (6, 8)): 35.549999999999955\n",
      "\t\tQ(s, a): Q((6, 7), (7, 7)): 34.549999999999955\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((5, 6), (5, 5)): 20.785354999999946\n",
      "\t\tQ(s, a): Q((5, 6), (4, 6)): 21.785354999999946\n",
      "\t\tQ(s, a): Q((5, 6), (6, 6)): 26.89549999999995\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((7, 6), (6, 6)): 26.89549999999995\n",
      "\t\tQ(s, a): Q((7, 6), (7, 7)): 34.549999999999955\n",
      "\t\tQ(s, a): Q((7, 6), (7, 5)): 26.08549999999995\n",
      "\t\tQ(s, a): Q((7, 6), (8, 6)): 27.08549999999995\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((4, 7), (4, 8)): 27.79549999999995\n",
      "\t\tQ(s, a): Q((4, 7), (4, 6)): 21.785354999999946\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((4, 8), (3, 8)): 24.01594999999995\n",
      "\t\tQ(s, a): Q((4, 8), (5, 8)): 31.99499999999995\n",
      "\t\tQ(s, a): Q((4, 8), (4, 7)): 24.01594999999995\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((3, 8), (2, 8)): 20.61435499999995\n",
      "\t\tQ(s, a): Q((3, 8), (4, 8)): 27.79549999999995\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((2, 8), (1, 8)): 17.55291949999995\n",
      "\t\tQ(s, a): Q((2, 8), (3, 8)): 24.01594999999995\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((1, 8), (2, 8)): 20.61435499999995\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((5, 8), (4, 8)): 27.79549999999995\n",
      "\t\tQ(s, a): Q((5, 8), (6, 8)): 35.549999999999955\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((6, 8), (6, 7)): 30.99499999999995\n",
      "\t\tQ(s, a): Q((6, 8), (5, 8)): 31.99499999999995\n",
      "\t\tQ(s, a): Q((6, 8), (7, 8)): 39.499999999999964\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((7, 8), (8, 8)): 44.99999999999997\n",
      "\t\tQ(s, a): Q((7, 8), (7, 7)): 34.549999999999955\n",
      "\t\tQ(s, a): Q((7, 8), (6, 8)): 35.549999999999955\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((8, 8), (7, 8)): 39.499999999999964\n",
      "\t\tQ(s, a): Q((8, 8), (9, 8)): 49.999999999999986\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((4, 4), (4, 5)): 19.606819499999943\n",
      "\t\tQ(s, a): Q((4, 4), (3, 4)): 14.881523794999945\n",
      "\t\tQ(s, a): Q((4, 4), (5, 4)): 18.706819499999945\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((7, 7), (6, 7)): 30.99499999999995\n",
      "\t\tQ(s, a): Q((7, 7), (7, 6)): 30.094999999999953\n",
      "\t\tQ(s, a): Q((7, 7), (7, 8)): 39.499999999999964\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((7, 5), (6, 5)): 24.205949999999948\n",
      "\t\tQ(s, a): Q((7, 5), (7, 6)): 30.094999999999953\n",
      "\t\tQ(s, a): Q((7, 5), (8, 5)): 24.376949999999947\n",
      "\t\tQ(s, a): Q((7, 5), (7, 4)): 23.47694999999995\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((8, 6), (7, 6)): 30.094999999999953\n",
      "\t\tQ(s, a): Q((8, 6), (8, 5)): 24.376949999999947\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((8, 5), (7, 5)): 26.08549999999995\n",
      "\t\tQ(s, a): Q((8, 5), (8, 4)): 20.939254999999946\n",
      "\t\tQ(s, a): Q((8, 5), (8, 6)): 27.08549999999995\n",
      "\t------------------state   action   reward\n",
      "\t\tQ(s, a): Q((7, 4), (6, 4)): 20.785354999999946\n",
      "\t\tQ(s, a): Q((7, 4), (8, 4)): 20.939254999999946\n",
      "\t\tQ(s, a): Q((7, 4), (7, 5)): 26.08549999999995\n"
     ]
    }
   ],
   "source": [
    "QL_solver.dump_Qvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current state: [0, 4] -> action: [1, 4] \n",
      "\t  #   #   #   #   S   #   #   #   #   # \n",
      "\t  #  -1   0   0  @@   0  -1   #  -1   # \n",
      "\t  #  -1  -1   0  -1  -1  -1   #  -1   # \n",
      "\t  #  -1  -1  -1  -1  -1  -1   #  -1   # \n",
      "\t  #   #  -1   #   0   0   0  -1  -1   # \n",
      "\t  #   0  -1  -1   0  -1   0   #   0   # \n",
      "\t  #  -1   0   0  -1   0  -1  -1   0   # \n",
      "\t  #   #  -1   #   0  -1  -1  -1  -1   # \n",
      "\t  #  -1   0  -1  -1   0   0   #   0   # \n",
      "\t  #   #   #   #   #   #   #   #  50   # \n",
      "current step: 1 \t score: -1.0\n",
      "\n",
      "current state: [1, 4] -> action: [1, 5] \n",
      "\t  #   #   #   #   S   #   #   #   #   # \n",
      "\t  #  -1   0   0  -1  @@  -1   #  -1   # \n",
      "\t  #  -1  -1   0  -1  -1  -1   #  -1   # \n",
      "\t  #  -1  -1  -1  -1  -1  -1   #  -1   # \n",
      "\t  #   #  -1   #   0   0   0  -1  -1   # \n",
      "\t  #   0  -1  -1   0  -1   0   #   0   # \n",
      "\t  #  -1   0   0  -1   0  -1  -1   0   # \n",
      "\t  #   #  -1   #   0  -1  -1  -1  -1   # \n",
      "\t  #  -1   0  -1  -1   0   0   #   0   # \n",
      "\t  #   #   #   #   #   #   #   #  50   # \n",
      "current step: 2 \t score: -1.0\n",
      "\n",
      "current state: [1, 5] -> action: [2, 5] \n",
      "\t  #   #   #   #   S   #   #   #   #   # \n",
      "\t  #  -1   0   0  -1   0  -1   #  -1   # \n",
      "\t  #  -1  -1   0  -1  @@  -1   #  -1   # \n",
      "\t  #  -1  -1  -1  -1  -1  -1   #  -1   # \n",
      "\t  #   #  -1   #   0   0   0  -1  -1   # \n",
      "\t  #   0  -1  -1   0  -1   0   #   0   # \n",
      "\t  #  -1   0   0  -1   0  -1  -1   0   # \n",
      "\t  #   #  -1   #   0  -1  -1  -1  -1   # \n",
      "\t  #  -1   0  -1  -1   0   0   #   0   # \n",
      "\t  #   #   #   #   #   #   #   #  50   # \n",
      "current step: 3 \t score: -2.0\n",
      "\n",
      "current state: [2, 5] -> action: [3, 5] \n",
      "\t  #   #   #   #   S   #   #   #   #   # \n",
      "\t  #  -1   0   0  -1   0  -1   #  -1   # \n",
      "\t  #  -1  -1   0  -1  -1  -1   #  -1   # \n",
      "\t  #  -1  -1  -1  -1  @@  -1   #  -1   # \n",
      "\t  #   #  -1   #   0   0   0  -1  -1   # \n",
      "\t  #   0  -1  -1   0  -1   0   #   0   # \n",
      "\t  #  -1   0   0  -1   0  -1  -1   0   # \n",
      "\t  #   #  -1   #   0  -1  -1  -1  -1   # \n",
      "\t  #  -1   0  -1  -1   0   0   #   0   # \n",
      "\t  #   #   #   #   #   #   #   #  50   # \n",
      "current step: 4 \t score: -3.0\n",
      "\n",
      "current state: [3, 5] -> action: [4, 5] \n",
      "\t  #   #   #   #   S   #   #   #   #   # \n",
      "\t  #  -1   0   0  -1   0  -1   #  -1   # \n",
      "\t  #  -1  -1   0  -1  -1  -1   #  -1   # \n",
      "\t  #  -1  -1  -1  -1  -1  -1   #  -1   # \n",
      "\t  #   #  -1   #   0  @@   0  -1  -1   # \n",
      "\t  #   0  -1  -1   0  -1   0   #   0   # \n",
      "\t  #  -1   0   0  -1   0  -1  -1   0   # \n",
      "\t  #   #  -1   #   0  -1  -1  -1  -1   # \n",
      "\t  #  -1   0  -1  -1   0   0   #   0   # \n",
      "\t  #   #   #   #   #   #   #   #  50   # \n",
      "current step: 5 \t score: -3.0\n",
      "\n",
      "current state: [4, 5] -> action: [4, 6] \n",
      "\t  #   #   #   #   S   #   #   #   #   # \n",
      "\t  #  -1   0   0  -1   0  -1   #  -1   # \n",
      "\t  #  -1  -1   0  -1  -1  -1   #  -1   # \n",
      "\t  #  -1  -1  -1  -1  -1  -1   #  -1   # \n",
      "\t  #   #  -1   #   0   0  @@  -1  -1   # \n",
      "\t  #   0  -1  -1   0  -1   0   #   0   # \n",
      "\t  #  -1   0   0  -1   0  -1  -1   0   # \n",
      "\t  #   #  -1   #   0  -1  -1  -1  -1   # \n",
      "\t  #  -1   0  -1  -1   0   0   #   0   # \n",
      "\t  #   #   #   #   #   #   #   #  50   # \n",
      "current step: 6 \t score: -3.0\n",
      "\n",
      "current state: [4, 6] -> action: [5, 6] \n",
      "\t  #   #   #   #   S   #   #   #   #   # \n",
      "\t  #  -1   0   0  -1   0  -1   #  -1   # \n",
      "\t  #  -1  -1   0  -1  -1  -1   #  -1   # \n",
      "\t  #  -1  -1  -1  -1  -1  -1   #  -1   # \n",
      "\t  #   #  -1   #   0   0   0  -1  -1   # \n",
      "\t  #   0  -1  -1   0  -1  @@   #   0   # \n",
      "\t  #  -1   0   0  -1   0  -1  -1   0   # \n",
      "\t  #   #  -1   #   0  -1  -1  -1  -1   # \n",
      "\t  #  -1   0  -1  -1   0   0   #   0   # \n",
      "\t  #   #   #   #   #   #   #   #  50   # \n",
      "current step: 7 \t score: -3.0\n",
      "\n",
      "current state: [5, 6] -> action: [6, 6] \n",
      "\t  #   #   #   #   S   #   #   #   #   # \n",
      "\t  #  -1   0   0  -1   0  -1   #  -1   # \n",
      "\t  #  -1  -1   0  -1  -1  -1   #  -1   # \n",
      "\t  #  -1  -1  -1  -1  -1  -1   #  -1   # \n",
      "\t  #   #  -1   #   0   0   0  -1  -1   # \n",
      "\t  #   0  -1  -1   0  -1   0   #   0   # \n",
      "\t  #  -1   0   0  -1   0  @@  -1   0   # \n",
      "\t  #   #  -1   #   0  -1  -1  -1  -1   # \n",
      "\t  #  -1   0  -1  -1   0   0   #   0   # \n",
      "\t  #   #   #   #   #   #   #   #  50   # \n",
      "current step: 8 \t score: -4.0\n",
      "\n",
      "current state: [6, 6] -> action: [6, 7] \n",
      "\t  #   #   #   #   S   #   #   #   #   # \n",
      "\t  #  -1   0   0  -1   0  -1   #  -1   # \n",
      "\t  #  -1  -1   0  -1  -1  -1   #  -1   # \n",
      "\t  #  -1  -1  -1  -1  -1  -1   #  -1   # \n",
      "\t  #   #  -1   #   0   0   0  -1  -1   # \n",
      "\t  #   0  -1  -1   0  -1   0   #   0   # \n",
      "\t  #  -1   0   0  -1   0  -1  @@   0   # \n",
      "\t  #   #  -1   #   0  -1  -1  -1  -1   # \n",
      "\t  #  -1   0  -1  -1   0   0   #   0   # \n",
      "\t  #   #   #   #   #   #   #   #  50   # \n",
      "current step: 9 \t score: -5.0\n",
      "\n",
      "current state: [6, 7] -> action: [6, 8] \n",
      "\t  #   #   #   #   S   #   #   #   #   # \n",
      "\t  #  -1   0   0  -1   0  -1   #  -1   # \n",
      "\t  #  -1  -1   0  -1  -1  -1   #  -1   # \n",
      "\t  #  -1  -1  -1  -1  -1  -1   #  -1   # \n",
      "\t  #   #  -1   #   0   0   0  -1  -1   # \n",
      "\t  #   0  -1  -1   0  -1   0   #   0   # \n",
      "\t  #  -1   0   0  -1   0  -1  -1  @@   # \n",
      "\t  #   #  -1   #   0  -1  -1  -1  -1   # \n",
      "\t  #  -1   0  -1  -1   0   0   #   0   # \n",
      "\t  #   #   #   #   #   #   #   #  50   # \n",
      "current step: 10 \t score: -5.0\n",
      "\n",
      "current state: [6, 8] -> action: [7, 8] \n",
      "\t  #   #   #   #   S   #   #   #   #   # \n",
      "\t  #  -1   0   0  -1   0  -1   #  -1   # \n",
      "\t  #  -1  -1   0  -1  -1  -1   #  -1   # \n",
      "\t  #  -1  -1  -1  -1  -1  -1   #  -1   # \n",
      "\t  #   #  -1   #   0   0   0  -1  -1   # \n",
      "\t  #   0  -1  -1   0  -1   0   #   0   # \n",
      "\t  #  -1   0   0  -1   0  -1  -1   0   # \n",
      "\t  #   #  -1   #   0  -1  -1  -1  @@   # \n",
      "\t  #  -1   0  -1  -1   0   0   #   0   # \n",
      "\t  #   #   #   #   #   #   #   #  50   # \n",
      "current step: 11 \t score: -6.0\n",
      "\n",
      "current state: [7, 8] -> action: [8, 8] \n",
      "\t  #   #   #   #   S   #   #   #   #   # \n",
      "\t  #  -1   0   0  -1   0  -1   #  -1   # \n",
      "\t  #  -1  -1   0  -1  -1  -1   #  -1   # \n",
      "\t  #  -1  -1  -1  -1  -1  -1   #  -1   # \n",
      "\t  #   #  -1   #   0   0   0  -1  -1   # \n",
      "\t  #   0  -1  -1   0  -1   0   #   0   # \n",
      "\t  #  -1   0   0  -1   0  -1  -1   0   # \n",
      "\t  #   #  -1   #   0  -1  -1  -1  -1   # \n",
      "\t  #  -1   0  -1  -1   0   0   #  @@   # \n",
      "\t  #   #   #   #   #   #   #   #  50   # \n",
      "current step: 12 \t score: -6.0\n",
      "\n",
      "current state: [8, 8] -> action: [9, 8] \n",
      "\t  #   #   #   #   S   #   #   #   #   # \n",
      "\t  #  -1   0   0  -1   0  -1   #  -1   # \n",
      "\t  #  -1  -1   0  -1  -1  -1   #  -1   # \n",
      "\t  #  -1  -1  -1  -1  -1  -1   #  -1   # \n",
      "\t  #   #  -1   #   0   0   0  -1  -1   # \n",
      "\t  #   0  -1  -1   0  -1   0   #   0   # \n",
      "\t  #  -1   0   0  -1   0  -1  -1   0   # \n",
      "\t  #   #  -1   #   0  -1  -1  -1  -1   # \n",
      "\t  #  -1   0  -1  -1   0   0   #   0   # \n",
      "\t  #   #   #   #   #   #   #   #  @@   # \n",
      "current step: 13 \t score: 44.0\n",
      "\n",
      "Goal!\n"
     ]
    }
   ],
   "source": [
    "QL_solver.qlearn(greedy_flg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
